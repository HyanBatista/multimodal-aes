{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer, ViTModel, ViTImageProcessor, Trainer, TrainingArguments\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    bert_model_name = 'neuralmind/bert-base-portuguese-cased'\n",
    "    vit_model_name = 'google/vit-base-patch16-224'\n",
    "    num_classes = 5\n",
    "    learning_rate = 2e-5\n",
    "    batch_size = 16\n",
    "    num_epochs = 10\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer: BertTokenizer, processor: ViTImageProcessor, label: str):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root = Path(csv_file).parent.as_posix()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.processor = processor\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def process_text(self, text: str | list[str]):\n",
    "        text_encoding = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        return text_encoding['input_ids'], text_encoding['attention_mask']\n",
    "\n",
    "    def process_image(self, image_path: str | list[str]):\n",
    "        if isinstance(image_path, str):\n",
    "            image_path = [image_path]\n",
    "        images = [Image.open(path).convert('RGB') for path in image_path]\n",
    "        pixel_values = self.processor(images=images, return_tensors='pt')['pixel_values']\n",
    "        return pixel_values\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, int):\n",
    "            idx = [idx]\n",
    "        text = self.data['text'].iloc[idx].to_list()\n",
    "        image_path = (self.root + '/' + self.data['image_path'].iloc[idx]).to_list()\n",
    "        labels = torch.tensor(self.data[self.label].iloc[idx].to_list(), dtype=torch.float)\n",
    "        input_ids, attention_mask = self.process_text(text)\n",
    "        pixel_values = self.process_image(image_path)\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': labels\n",
    "        }\n",
    "    __getitems__ = __getitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(config.bert_model_name)\n",
    "        self.vit = ViTModel.from_pretrained(config.vit_model_name)\n",
    "\n",
    "        # Congelando os parâmetros do BERT e do ViT\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.vit.parameters():\n",
    "            param.requires_grad = False\n",
    "        hidden_size = self.bert.config.hidden_size + self.vit.config.hidden_size\n",
    "        # Camada Fully Connected para classificação\n",
    "        self.fc = nn.Linear(hidden_size, config.num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        # Obtenção das saídas do BERT\n",
    "        text_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Obtenção das saídas do ViT\n",
    "        image_output = self.vit(pixel_values=pixel_values).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Concatenando as saídas do BERT e do ViT\n",
    "        combined = torch.cat((text_output, image_output), dim=1)\n",
    "\n",
    "        # Classificador final\n",
    "        logits = self.fc(combined)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(config.bert_model_name)\n",
    "processor = ViTImageProcessor.from_pretrained(config.vit_model_name)\n",
    "\n",
    "# Carregamento do Dataset\n",
    "dataset = MultimodalDataset(\n",
    "    csv_file='data/MEC/mec-dataset.csv',\n",
    "    tokenizer=tokenizer,\n",
    "    processor=processor,\n",
    "    label='formal_register'\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True, collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultimodalModel(config)\n",
    "criterion = nn.MSELoss()  # Loss para regressão\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(config.num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs_ids, attention_mask, images, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs_ids, attention_mask, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass e otimização\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs_ids.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{config.num_epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
