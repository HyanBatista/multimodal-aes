{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyanbatista42/miniconda3/envs/multimodal-aes/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    cohen_kappa_score,\n",
    "    f1_score,\n",
    "    make_scorer,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.metrics._scorer import _BaseScorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"../\"\n",
    "DATASET = os.path.join(ROOT, \"data/br-pt-narrative-essays.csv\")\n",
    "SCORING = {\n",
    "    \"acc\": make_scorer(accuracy_score),\n",
    "    \"macro_prec\": make_scorer(precision_score, average=\"macro\"),\n",
    "    \"weighted_prec\": make_scorer(precision_score, average=\"weighted\"),\n",
    "    \"macro_recall\": make_scorer(recall_score, average=\"macro\"),\n",
    "    \"weighted_recall\": make_scorer(recall_score, average=\"weighted\"),\n",
    "    \"macro_f1\": make_scorer(f1_score, average=\"macro\"),\n",
    "    \"weighted_f1\": make_scorer(f1_score, average=\"weighted\"),\n",
    "    \"kappa\": make_scorer(cohen_kappa_score)\n",
    "}\n",
    "CV = 5\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"neuralmind/bert-base-portuguese-cased\",\n",
    "        max_length=128,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Tokenize and encode the text, and get CLS token embedding for classification tasks\"\"\"\n",
    "\n",
    "        X = list(X)\n",
    "\n",
    "        if not isinstance(X, list):\n",
    "            raise ValueError(\"Not a list of strings\")\n",
    "        elif not all(isinstance(x, str) for x in X):\n",
    "            raise ValueError(\"Not all instances are strings.\")\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            X,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "\n",
    "def scores_to_df(scores: tuple[tuple[str, str, float]]) -> pd.DataFrame:\n",
    "    \"\"\"Transform the scores dictionary into a dataframe object.\"\"\"\n",
    "    data = []\n",
    "    for score in scores:\n",
    "        data.append(\n",
    "            {\n",
    "                \"algo\": score[\"algo\"],\n",
    "                \"encoder\": score[\"encoder\"],\n",
    "                \"acc\": np.mean(score[\"test_acc\"]),\n",
    "                \"macro_prec\": np.mean(score[\"test_macro_prec\"]),\n",
    "                \"weighted_prec\": np.mean(score[\"test_weighted_prec\"]),\n",
    "                \"macro_recall\": np.mean(score[\"test_macro_recall\"]),\n",
    "                \"weighted_recall\": np.mean(score[\"test_weighted_recall\"]),\n",
    "                \"macro_f1\": np.mean(score[\"test_macro_f1\"]),\n",
    "                \"weighted_f1\": np.mean(score[\"test_weighted_f1\"]),\n",
    "                \"kappa\": np.mean(score[\"test_kappa\"]),\n",
    "            }\n",
    "        )\n",
    "    df = pd.DataFrame(data=data)\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Remove unwanted tokens from the text.\"\"\"\n",
    "    text = re.sub(r\"\\n\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def _create_tfidf_pipeline(clf: BaseEstimator) -> Pipeline:\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"encoder\", TfidfVectorizer()),\n",
    "            (\"clf\", clf),\n",
    "        ]\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def _create_bert_pipeline(clf: BaseEstimator) -> Pipeline:\n",
    "    pipeline = Pipeline([(\"encoder\", BERTEncoder()), (\"clf\", clf)])\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def run_sklearn_experiments(\n",
    "    clfs: list[BaseEstimator],\n",
    "    df: pd.DataFrame,\n",
    "    scoring: dict[str, _BaseScorer],\n",
    "    cv: int,\n",
    "    preprocessors: list[Any],\n",
    "    feature_column: str,\n",
    "    target_column: str,\n",
    "    n_jobs: int = 1,\n",
    "):\n",
    "    \"\"\"Run Scikit-Learn based experiments.\"\"\"\n",
    "\n",
    "    ALGO_NAME_DICT = {\n",
    "        ExtraTreesClassifier: \"Extra Trees\",\n",
    "        DecisionTreeClassifier: \"Decision Tree\",\n",
    "        SVC: \"Support Vector\",\n",
    "        RandomForestClassifier: \"Random Forest\",\n",
    "    }\n",
    "\n",
    "    ENCODER_NAME_DICT = {\n",
    "        TfidfVectorizer: \"TF-IDF\",\n",
    "        BERTEncoder: \"BERT\",\n",
    "    }\n",
    "\n",
    "    def _run_sklearn_cv(\n",
    "        clf: BaseEstimator,\n",
    "        X: pd.DataFrame,\n",
    "        y: pd.DataFrame,\n",
    "        scoring: dict[str, _BaseScorer],\n",
    "        cv: int,\n",
    "    ):\n",
    "        \"\"\"Run cross-validation on a Scikit-Learn estimator.\"\"\"\n",
    "\n",
    "        assert issubclass(type(clf), BaseEstimator) is True\n",
    "\n",
    "        cv_results = cross_validate(\n",
    "            clf,\n",
    "            X,\n",
    "            y,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=n_jobs,\n",
    "        )\n",
    "        cv_results[\"algo\"] = ALGO_NAME_DICT[type(clf.named_steps[\"clf\"])]\n",
    "        cv_results[\"encoder\"] = ENCODER_NAME_DICT[type(clf.named_steps[\"encoder\"])]\n",
    "        return cv_results\n",
    "\n",
    "    cv_results_list = []\n",
    "\n",
    "    # Apply preprocessors\n",
    "    for preprocessor in preprocessors:\n",
    "        df[feature_column] = df[feature_column].apply(\n",
    "            lambda feature: preprocessor(feature)\n",
    "        )\n",
    "\n",
    "    for clf in clfs:\n",
    "        # Create pipelines\n",
    "        tfidf_pipeline = _create_tfidf_pipeline(clf)\n",
    "        bert_pipeline = _create_bert_pipeline(clf)\n",
    "\n",
    "        # Run CV using the pipelines\n",
    "        tfidf_cv_results_list = _run_sklearn_cv(\n",
    "            tfidf_pipeline,\n",
    "            df[feature_column],\n",
    "            df[target_column],\n",
    "            scoring,\n",
    "            cv,\n",
    "        )\n",
    "\n",
    "        bert_cv_results_list = _run_sklearn_cv(\n",
    "            bert_pipeline,\n",
    "            df[feature_column],\n",
    "            df[target_column],\n",
    "            scoring,\n",
    "            cv,\n",
    "        )\n",
    "\n",
    "        # Join results\n",
    "        joint_cv_results_list = tfidf_cv_results_list + bert_cv_results_list\n",
    "        cv_results_list.append(joint_cv_results_list)\n",
    "\n",
    "    return cv_results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>motivating_situation</th>\n",
       "      <th>image_url</th>\n",
       "      <th>image_path</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>thematic_coherence</th>\n",
       "      <th>formal_register</th>\n",
       "      <th>text_typology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>( O chorrinho nino                  )\\n-\\nEu e...</td>\n",
       "      <td>Eu encontrei em cima do armário alguns potes c...</td>\n",
       "      <td>https://storage.googleapis.com/ciclos-10698-bu...</td>\n",
       "      <td>MEC/Rc7dMxTP7ZdLNEvmF0jo/iet1QFw2ARNkv9Hx6KGe/...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.As meninas do potes de Tintas [T]\\nUma vez eu...</td>\n",
       "      <td>Eu encontrei em cima do armário alguns potes c...</td>\n",
       "      <td>https://storage.googleapis.com/ciclos-10698-bu...</td>\n",
       "      <td>MEC/Rc7dMxTP7ZdLNEvmF0jo/F80gTOBoh2Lk5JtrLWUO/...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 uma menina encontrou varios potes de tinta  ...</td>\n",
       "      <td>Eu encontrei em cima do armário alguns potes c...</td>\n",
       "      <td>https://storage.googleapis.com/ciclos-10698-bu...</td>\n",
       "      <td>MEC/Rc7dMxTP7ZdLNEvmF0jo/HjnamZPzaZcDeEANF2IC/...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011 [T] A menina da (artes). \\n\\nDePois que e...</td>\n",
       "      <td>Eu encontrei em cima do armário alguns potes c...</td>\n",
       "      <td>https://storage.googleapis.com/ciclos-10698-bu...</td>\n",
       "      <td>MEC/Rc7dMxTP7ZdLNEvmF0jo/r3DUtFJn9twSvqcNloNM/...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28/03/2022\\n[T] A cobra felena\\nem uma tarde m...</td>\n",
       "      <td>Eu encontrei em cima do armário alguns potes c...</td>\n",
       "      <td>https://storage.googleapis.com/ciclos-10698-bu...</td>\n",
       "      <td>MEC/Rc7dMxTP7ZdLNEvmF0jo/tKZIn59KtOcbe9z0n6oY/...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  ( O chorrinho nino                  )\\n-\\nEu e...   \n",
       "1  .As meninas do potes de Tintas [T]\\nUma vez eu...   \n",
       "2  1 uma menina encontrou varios potes de tinta  ...   \n",
       "3  2011 [T] A menina da (artes). \\n\\nDePois que e...   \n",
       "4  28/03/2022\\n[T] A cobra felena\\nem uma tarde m...   \n",
       "\n",
       "                                motivating_situation  \\\n",
       "0  Eu encontrei em cima do armário alguns potes c...   \n",
       "1  Eu encontrei em cima do armário alguns potes c...   \n",
       "2  Eu encontrei em cima do armário alguns potes c...   \n",
       "3  Eu encontrei em cima do armário alguns potes c...   \n",
       "4  Eu encontrei em cima do armário alguns potes c...   \n",
       "\n",
       "                                           image_url  \\\n",
       "0  https://storage.googleapis.com/ciclos-10698-bu...   \n",
       "1  https://storage.googleapis.com/ciclos-10698-bu...   \n",
       "2  https://storage.googleapis.com/ciclos-10698-bu...   \n",
       "3  https://storage.googleapis.com/ciclos-10698-bu...   \n",
       "4  https://storage.googleapis.com/ciclos-10698-bu...   \n",
       "\n",
       "                                          image_path  cohesion  \\\n",
       "0  MEC/Rc7dMxTP7ZdLNEvmF0jo/iet1QFw2ARNkv9Hx6KGe/...         3   \n",
       "1  MEC/Rc7dMxTP7ZdLNEvmF0jo/F80gTOBoh2Lk5JtrLWUO/...         3   \n",
       "2  MEC/Rc7dMxTP7ZdLNEvmF0jo/HjnamZPzaZcDeEANF2IC/...         3   \n",
       "3  MEC/Rc7dMxTP7ZdLNEvmF0jo/r3DUtFJn9twSvqcNloNM/...         3   \n",
       "4  MEC/Rc7dMxTP7ZdLNEvmF0jo/tKZIn59KtOcbe9z0n6oY/...         4   \n",
       "\n",
       "   thematic_coherence  formal_register  text_typology  \n",
       "0                   3                3              4  \n",
       "1                   3                3              4  \n",
       "2                   3                3              4  \n",
       "3                   3                3              4  \n",
       "4                   1                4              4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATASET, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a value is a string\n",
    "def is_string(value):\n",
    "    return isinstance(value, str)\n",
    "\n",
    "# Apply the function and filter the DataFrame\n",
    "df_filtered = df[df['text'].apply(is_string)]\n",
    "\n",
    "# Reset index if needed\n",
    "df_filtered.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'dict' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39mUndefinedMetricWarning)\n\u001b[1;32m     10\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_sklearn_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_filtered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSCORING\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mclean_text\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcohesion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 158\u001b[0m, in \u001b[0;36mrun_sklearn_experiments\u001b[0;34m(clfs, df, scoring, cv, preprocessors, feature_column, target_column, n_jobs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     bert_cv_results_list \u001b[38;5;241m=\u001b[39m _run_sklearn_cv(\n\u001b[1;32m    150\u001b[0m         bert_pipeline,\n\u001b[1;32m    151\u001b[0m         df[feature_column],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m         cv,\n\u001b[1;32m    155\u001b[0m     )\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Join results\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     joint_cv_results_list \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_cv_results_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbert_cv_results_list\u001b[49m\n\u001b[1;32m    159\u001b[0m     cv_results_list\u001b[38;5;241m.\u001b[39mappend(joint_cv_results_list)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results_list\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'dict' and 'dict'"
     ]
    }
   ],
   "source": [
    "clfs = [\n",
    "    SVC(),\n",
    "    RandomForestClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    ExtraTreesClassifier()\n",
    "]\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(action=\"ignore\", category=UndefinedMetricWarning)\n",
    "    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "    cv_results = run_sklearn_experiments(clfs, df_filtered, scoring=SCORING, cv=CV, preprocessors=[clean_text], feature_column=\"text\", target_column=\"cohesion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algo</th>\n",
       "      <th>encoder</th>\n",
       "      <th>acc</th>\n",
       "      <th>macro_prec</th>\n",
       "      <th>weighted_prec</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>weighted_recall</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Support Vector</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.681825</td>\n",
       "      <td>0.259292</td>\n",
       "      <td>0.544278</td>\n",
       "      <td>0.206309</td>\n",
       "      <td>0.681825</td>\n",
       "      <td>0.175538</td>\n",
       "      <td>0.560300</td>\n",
       "      <td>0.020110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.666670</td>\n",
       "      <td>0.299165</td>\n",
       "      <td>0.530862</td>\n",
       "      <td>0.228474</td>\n",
       "      <td>0.666670</td>\n",
       "      <td>0.217940</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.031691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.557256</td>\n",
       "      <td>0.296601</td>\n",
       "      <td>0.543236</td>\n",
       "      <td>0.276714</td>\n",
       "      <td>0.557256</td>\n",
       "      <td>0.281307</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.079544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.651502</td>\n",
       "      <td>0.250197</td>\n",
       "      <td>0.518574</td>\n",
       "      <td>0.225143</td>\n",
       "      <td>0.651502</td>\n",
       "      <td>0.212490</td>\n",
       "      <td>0.559056</td>\n",
       "      <td>0.021207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             algo encoder       acc  macro_prec  weighted_prec  macro_recall  \\\n",
       "0  Support Vector  TF-IDF  0.681825    0.259292       0.544278      0.206309   \n",
       "1   Random Forest  TF-IDF  0.666670    0.299165       0.530862      0.228474   \n",
       "2   Decision Tree  TF-IDF  0.557256    0.296601       0.543236      0.276714   \n",
       "3     Extra Trees  TF-IDF  0.651502    0.250197       0.518574      0.225143   \n",
       "\n",
       "   weighted_recall  macro_f1  weighted_f1     kappa  \n",
       "0         0.681825  0.175538     0.560300  0.020110  \n",
       "1         0.666670  0.217940     0.565789  0.031691  \n",
       "2         0.557256  0.281307     0.548780  0.079544  \n",
       "3         0.651502  0.212490     0.559056  0.021207  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results_df = scores_to_df(cv_results)\n",
    "cv_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgboost_experiments(clf, X: pd.DataFrame, y: pd.DataFrame, random_state: int = 42):\n",
    "    \"\"\"Run XGBoost based experiments.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_catboost_experiments(clf, X: pd.DataFrame, y: pd.DataFrame, random_state: int = 42):\n",
    "    \"\"\"Run CatBoost based experiments.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal-aes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
