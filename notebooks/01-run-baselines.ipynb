{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from typing import Any\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage as ski\n",
    "import torch\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    cohen_kappa_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import BertModel, BertTokenizer, ViTFeatureExtractor, ViTModel\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"../data/\"\n",
    "DATASET = os.path.join(ROOT, \"br-pt-narrative-essays.csv\")\n",
    "CV = 5\n",
    "SEED = 45\n",
    "COMPETENCE = \"text_typology\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"neuralmind/bert-base-portuguese-cased\",\n",
    "        max_length=128,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Tokenize and encode the text, and get CLS token embedding for classification tasks\"\"\"\n",
    "\n",
    "        X = list(X)\n",
    "\n",
    "        if not isinstance(X, list):\n",
    "            raise ValueError(\"Not a list of strings\")\n",
    "        elif not all(isinstance(x, str) for x in X):\n",
    "            raise ValueError(\"Not all instances are strings.\")\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            X,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "class ViTEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"google/vit-base-patch16-224\",\n",
    "        image_size=128,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "        self.model = ViTModel.from_pretrained(model_name, add_pooling_layer=False)\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Extract features from images using ViT\"\"\"\n",
    "\n",
    "        X = list(X)\n",
    "\n",
    "        if not isinstance(X, list):\n",
    "            raise ValueError(\"Input is not a list of images.\")\n",
    "        elif not all(isinstance(x, np.ndarray) for x in X):\n",
    "            raise ValueError(\"Not all instances are numpy arrays (images).\")\n",
    "\n",
    "        inputs = self.feature_extractor(\n",
    "            X,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "\n",
    "class LBPEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"LBP encoder for image data.\"\"\"\n",
    "\n",
    "    def __init__(self, radius: int = 1, sampling_pixels: int = 106):\n",
    "        self.radius = radius\n",
    "        self.sampling_pixels = sampling_pixels\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Extract the LBP from the images batch.\"\"\"\n",
    "        logger.debug(\"Encoding images...\")\n",
    "        X = list(X)\n",
    "        logger.debug(\"Converting...\")\n",
    "        cvt_imgs = [self._cvt(img) for img in X]\n",
    "        logger.debug(\"Running LBP algorithm...\")\n",
    "        imgs_lbps = [self._get_lbp(img) for img in cvt_imgs]\n",
    "        logger.debug(\"Getting the histograms...\")\n",
    "        imgs_hists = [self._get_hist(img_lbp) for img_lbp in imgs_lbps]\n",
    "        logger.debug(\"Extracting LBP features from histograms...\")\n",
    "        features = self._get_features(imgs_hists)\n",
    "        logger.debug(\"Finished with encoding images\")\n",
    "\n",
    "        return features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def _cvt(self, img):\n",
    "        if len(img.shape) > 2:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "        i_min = np.min(img)\n",
    "        i_max = np.max(img)\n",
    "        if i_max - i_min != 0:\n",
    "            img = (img - i_min) / (i_max - i_min)\n",
    "\n",
    "        img = img.astype(np.uint8)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def _get_lbp(self, img):\n",
    "        lbp = ski.feature.local_binary_pattern(\n",
    "            img, self.sampling_pixels, self.radius, method=\"uniform\"\n",
    "        )\n",
    "        return (img, lbp)\n",
    "\n",
    "    def _get_hist(self, img_lbp):\n",
    "        img, lbp = img_lbp\n",
    "        hist, _ = np.histogram(\n",
    "            lbp.ravel(),\n",
    "            bins=np.arange(0, self.sampling_pixels + 3),\n",
    "            range=(0, self.sampling_pixels + 2),\n",
    "        )\n",
    "        hist = hist.astype(\"float\")\n",
    "        hist /= hist.sum() + 1e-6\n",
    "        return img, hist\n",
    "    \n",
    "    def _get_features(self, imgs_hists):\n",
    "        hists = [img_hist[1] for img_hist in imgs_hists]\n",
    "        features = []\n",
    "        for h in hists:\n",
    "            features.extend(h)\n",
    "        return hists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_tfidf_pipeline(clf: BaseEstimator) -> Pipeline:\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"encoder\", TfidfVectorizer()),\n",
    "            (\"clf\", clf),\n",
    "        ]\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def _create_lbp_pipeline(clf: BaseEstimator) -> Pipeline:\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"encoder\", LBPEncoder()),\n",
    "            (\"clf\", clf),\n",
    "        ]\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def _create_bert_pipeline(clf: BaseEstimator) -> Pipeline:\n",
    "    pipeline = Pipeline([(\"encoder\", BERTEncoder()), (\"clf\", clf)])\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def _create_vit_pipeline(clf: BaseEstimator) -> Pipeline:\n",
    "    pipeline = Pipeline([(\"encoder\", ViTEncoder()), (\"clf\", clf)])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_to_df(scores: tuple[tuple[str, str, float]]) -> pd.DataFrame:\n",
    "    \"\"\"Transform the scores dictionary into a dataframe object.\"\"\"\n",
    "    data = []\n",
    "    for score in scores:\n",
    "        data.append(\n",
    "            {\n",
    "                \"algo\": score[\"algo\"],\n",
    "                \"encoder\": score[\"encoder\"],\n",
    "                \"acc\": np.mean(score[\"test_acc\"]),\n",
    "                \"macro_prec\": np.mean(score[\"test_macro_prec\"]),\n",
    "                \"weighted_prec\": np.mean(score[\"test_weighted_prec\"]),\n",
    "                \"macro_recall\": np.mean(score[\"test_macro_recall\"]),\n",
    "                \"weighted_recall\": np.mean(score[\"test_weighted_recall\"]),\n",
    "                \"macro_f1\": np.mean(score[\"test_macro_f1\"]),\n",
    "                \"weighted_f1\": np.mean(score[\"test_weighted_f1\"]),\n",
    "                \"kappa\": np.mean(score[\"test_kappa\"]),\n",
    "            }\n",
    "        )\n",
    "    df = pd.DataFrame(data=data)\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Remove unwanted tokens from the text.\"\"\"\n",
    "\n",
    "    # Well-formed tags with format [<LETTER_OR_SYMBOL>]\n",
    "    tag_regex = r\"(\\[[PpSsTtXx?]\\])\"\n",
    "\n",
    "    # Well-formed tags with format {<LETTER_OR_SYMBOL>}\n",
    "    tag_regex += r\"|({[ptx?]})\"\n",
    "\n",
    "    # Well-formed tags [LT] or [LC]\n",
    "    tag_regex += r\"|(\\[L[TC]\\])\"\n",
    "\n",
    "    # Well-formed tags with format [lt] or [lc]\n",
    "    tag_regex += r\"|(\\[l[tc]\\])\"\n",
    "\n",
    "    # Variant with a trailing space\n",
    "    tag_regex += r\"|(\\[ P\\])\"\n",
    "\n",
    "    # Mixed closing/opening symbol\n",
    "    tag_regex += r\"|(\\[[PX?]\\})\"\n",
    "    tag_regex += r\"|(\\{?\\])\"\n",
    "\n",
    "    text = re.sub(tag_regex, \"\", text)\n",
    "    text = re.sub(r\"\\n\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def transform_image(image):\n",
    "    resized_image = cv2.resize(image, (128, 128))\n",
    "    # gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "    # _, binary_image = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    # return binary_image\n",
    "    return resized_image\n",
    "\n",
    "\n",
    "def run_experiments(\n",
    "    clfs: list[BaseEstimator],\n",
    "    df: pd.DataFrame,\n",
    "    cv: int,\n",
    "    preprocessors: list[Any],\n",
    "    text_feature_column: str,\n",
    "    image_feature_column: str,\n",
    "    target_column: str,\n",
    "    random_state: int,\n",
    "):\n",
    "    \"\"\"Run Scikit-Learn based experiments.\"\"\"\n",
    "\n",
    "    ALGO_NAME_DICT = {\n",
    "        ExtraTreesClassifier: \"Extra Trees\",\n",
    "        DecisionTreeClassifier: \"Decision Tree\",\n",
    "        SVC: \"Support Vector\",\n",
    "        RandomForestClassifier: \"Random Forest\",\n",
    "        CatBoostClassifier: \"CatBoost\",\n",
    "        XGBClassifier: \"XGBoost\",\n",
    "        MLPClassifier: \"MLP\",\n",
    "    }\n",
    "\n",
    "    ENCODER_NAME_DICT = {\n",
    "        TfidfVectorizer: \"TF-IDF\",\n",
    "        BERTEncoder: \"BERT\",\n",
    "        LBPEncoder: \"LBP\",\n",
    "        ViTEncoder: \"ViT\",\n",
    "    }\n",
    "\n",
    "    def _fix_missing_classes(X, y, dtype: str, all_classes: list):\n",
    "        \"\"\"Fix the problem of number of classes mismatching.\"\"\"\n",
    "        if y is None:\n",
    "            return X\n",
    "\n",
    "        unique_classes = np.unique(y)\n",
    "        missing_classes = np.setdiff1d(all_classes, unique_classes)\n",
    "\n",
    "        if len(missing_classes) > 0:\n",
    "            if isinstance(X, pd.Series):\n",
    "                # Convert Series to 2D array\n",
    "                X = X.to_numpy().reshape(-1, 1)\n",
    "            elif len(X.shape) == 1:\n",
    "                # Convert 1D array to 2D\n",
    "                X = X.reshape(-1, 1)\n",
    "\n",
    "            if len(X.shape) == 1:\n",
    "                X_extra = np.zeros((len(missing_classes), 1))\n",
    "                if dtype == \"text\":\n",
    "                    # Ensure dtype is handled correctly\n",
    "                    if isinstance(X_extra, np.ndarray):\n",
    "                        X_extra = np.array([\"\" for _ in range(len(missing_classes))], dtype=str).reshape(\n",
    "                            (len(missing_classes), 1)\n",
    "                        )\n",
    "            else:\n",
    "                X_extra = np.zeros((len(missing_classes), X.shape[1]))\n",
    "                if dtype == \"text\":\n",
    "                    if isinstance(X_extra, np.ndarray):\n",
    "                        X_extra = np.array([\"\" for _ in range(len(missing_classes))], dtype=str).reshape(\n",
    "                            (len(missing_classes), X.shape[1])\n",
    "                        )\n",
    "\n",
    "            y_extra = np.array(missing_classes)\n",
    "\n",
    "            if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
    "                X_extra = pd.DataFrame(X_extra, columns=X.columns)\n",
    "                X = pd.concat([pd.DataFrame(X, columns=X.columns), X_extra], axis=0)\n",
    "            else:\n",
    "                # Concatenate extra data\n",
    "                X = np.vstack([X, X_extra])\n",
    "\n",
    "            y = np.hstack([y, y_extra])\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def _run_sklearn_cv(\n",
    "        clf: BaseEstimator | Pipeline,\n",
    "        X: pd.DataFrame,\n",
    "        y: pd.DataFrame,\n",
    "        cv: int,\n",
    "        random_state: int,\n",
    "        feature_column: str,\n",
    "    ):\n",
    "        \"\"\"Run cross-validation on a Scikit-Learn estimator.\"\"\"\n",
    "\n",
    "        assert issubclass(type(clf), BaseEstimator) is True\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=cv, random_state=random_state, shuffle=True)\n",
    "\n",
    "        cv_results = {}\n",
    "        cv_results[\"test_acc\"] = []\n",
    "        cv_results[\"test_macro_prec\"] = []\n",
    "        cv_results[\"test_weighted_prec\"] = []\n",
    "        cv_results[\"test_macro_recall\"] = []\n",
    "        cv_results[\"test_weighted_recall\"] = []\n",
    "        cv_results[\"test_macro_f1\"] = []\n",
    "        cv_results[\"test_weighted_f1\"] = []\n",
    "        cv_results[\"test_kappa\"] = []\n",
    "\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "\n",
    "            # Create and balance datasets\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            X_train, y_train = RandomOverSampler(random_state=SEED).fit_resample(\n",
    "                X_train.to_frame(), y_train\n",
    "            )\n",
    "\n",
    "            # Fix missing classes\n",
    "            all_classes = np.arange(0, 5)\n",
    "            dtype = (\n",
    "                \"text\"\n",
    "                if type(clf.named_steps[\"encoder\"]) in [TfidfVectorizer, BERTEncoder]\n",
    "                else \"image\"\n",
    "            )\n",
    "            X_train, y_train = _fix_missing_classes(\n",
    "                X_train, y_train, dtype=dtype, all_classes=all_classes\n",
    "            )\n",
    "\n",
    "            # Train model\n",
    "            clf.fit(X_train[feature_column], y_train)\n",
    "\n",
    "            # Evaluate model\n",
    "            y_pred = clf.predict(X_test)\n",
    "            cv_results[\"test_acc\"].append(accuracy_score(y_test, y_pred))\n",
    "            cv_results[\"test_macro_prec\"].append(\n",
    "                precision_score(y_test, y_pred, average=\"macro\")\n",
    "            )\n",
    "            cv_results[\"test_weighted_prec\"].append(\n",
    "                precision_score(y_test, y_pred, average=\"weighted\")\n",
    "            )\n",
    "            cv_results[\"test_macro_recall\"].append(\n",
    "                recall_score(y_test, y_pred, average=\"macro\")\n",
    "            )\n",
    "            cv_results[\"test_weighted_recall\"].append(\n",
    "                recall_score(y_test, y_pred, average=\"weighted\")\n",
    "            )\n",
    "            cv_results[\"test_macro_f1\"].append(\n",
    "                recall_score(y_test, y_pred, average=\"macro\")\n",
    "            )\n",
    "            cv_results[\"test_weighted_f1\"].append(\n",
    "                f1_score(y_test, y_pred, average=\"weighted\")\n",
    "            )\n",
    "            cv_results[\"test_kappa\"].append(cohen_kappa_score(y_test, y_pred))\n",
    "\n",
    "        cv_results[\"algo\"] = ALGO_NAME_DICT[type(clf.named_steps[\"clf\"])]\n",
    "        cv_results[\"encoder\"] = ENCODER_NAME_DICT[type(clf.named_steps[\"encoder\"])]\n",
    "        return cv_results\n",
    "\n",
    "    cv_results_list = []\n",
    "\n",
    "    # Apply preprocessors\n",
    "    for preprocessor in preprocessors[\"image\"]:\n",
    "        df[image_feature_column] = df[image_feature_column].apply(\n",
    "            lambda feature: preprocessor(cv2.imread(feature))\n",
    "        )\n",
    "\n",
    "    for preprocessor in preprocessors[\"text\"]:\n",
    "        df[text_feature_column] = df[text_feature_column].apply(\n",
    "            lambda feature: preprocessor(feature)\n",
    "        )\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[target_column] = label_encoder.fit_transform(df[target_column])\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    for i, clf in enumerate(tqdm(clfs, desc=\"Classifiers\")):\n",
    "        clf_name = ALGO_NAME_DICT[type(clf)]\n",
    "        logger.debug(f\"#{i + 1} classifier: {clf_name}\")\n",
    "\n",
    "        # Create pipelines\n",
    "        tfidf_pipeline = _create_tfidf_pipeline(clf)\n",
    "        bert_pipeline = _create_bert_pipeline(clf)\n",
    "        lbp_pipeline = _create_lbp_pipeline(clf)\n",
    "        vit_pipeline = _create_vit_pipeline(clf)\n",
    "\n",
    "        # Run CV using the pipelines\n",
    "        logger.debug(f\"Running TF-IDF pipeline #{i + 1}\")\n",
    "        tfidf_cv_results = _run_sklearn_cv(\n",
    "            tfidf_pipeline,\n",
    "            df[text_feature_column],\n",
    "            df[target_column],\n",
    "            cv,\n",
    "            random_state,\n",
    "            feature_column=text_feature_column,\n",
    "        )\n",
    "        logger.debug(f\"Ending TF-IDF pipeline #{i + 1}\")\n",
    "\n",
    "        logger.debug(f\"Running BERT pipeline #{i + 1}\")\n",
    "        bert_cv_results = _run_sklearn_cv(\n",
    "            bert_pipeline,\n",
    "            df[text_feature_column],\n",
    "            df[target_column],\n",
    "            cv,\n",
    "            random_state,\n",
    "            feature_column=text_feature_column,\n",
    "        )\n",
    "        logger.debug(f\"Ending BERT pipeline #{i + 1}\")\n",
    "\n",
    "        # logger.debug(f\"Running LBP pipeline #{i + 1}\")\n",
    "        lbp_cv_results = _run_sklearn_cv(\n",
    "            lbp_pipeline,\n",
    "            df[image_feature_column],\n",
    "            df[target_column],\n",
    "            cv,\n",
    "            random_state,\n",
    "            feature_column=image_feature_column,\n",
    "        )\n",
    "        logger.debug(f\"Ending LBP pipeline #{i + 1}\")\n",
    "\n",
    "        logger.debug(f\"Running ViT pipeline #{i + 1}\")\n",
    "        vit_cv_results = _run_sklearn_cv(\n",
    "            vit_pipeline,\n",
    "            df[image_feature_column],\n",
    "            df[target_column],\n",
    "            cv,\n",
    "            random_state,\n",
    "            feature_column=image_feature_column,\n",
    "        )\n",
    "        logger.debug(f\"Ending ViT pipeline #{i + 1}\")\n",
    "\n",
    "        # Append results\n",
    "        cv_results_list.append(tfidf_cv_results)\n",
    "        cv_results_list.append(bert_cv_results)\n",
    "        cv_results_list.append(lbp_cv_results)\n",
    "        cv_results_list.append(vit_cv_results)\n",
    "\n",
    "    return cv_results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>motivating_situation</th>\n",
       "      <th>image_url</th>\n",
       "      <th>image_path</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>thematic_coherence</th>\n",
       "      <th>formal_register</th>\n",
       "      <th>text_typology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>( O chorrinho nino                  )\\n-\\nEu e...</td>\n",
       "      <td>Eu encontrei em cima do armário alguns potes c...</td>\n",
       "      <td>https://storage.googleapis.com/ciclos-10698-bu...</td>\n",
       "      <td>MEC/Rc7dMxTP7ZdLNEvmF0jo/iet1QFw2ARNkv9Hx6KGe/...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.As meninas do potes de Tintas [T]\\nUma vez eu...</td>\n",
       "      <td>Eu encontrei em cima do armário alguns potes c...</td>\n",
       "      <td>https://storage.googleapis.com/ciclos-10698-bu...</td>\n",
       "      <td>MEC/Rc7dMxTP7ZdLNEvmF0jo/F80gTOBoh2Lk5JtrLWUO/...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 uma menina encontrou varios potes de tinta  ...</td>\n",
       "      <td>Eu encontrei em cima do armário alguns potes c...</td>\n",
       "      <td>https://storage.googleapis.com/ciclos-10698-bu...</td>\n",
       "      <td>MEC/Rc7dMxTP7ZdLNEvmF0jo/HjnamZPzaZcDeEANF2IC/...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011 [T] A menina da (artes). \\n\\nDePois que e...</td>\n",
       "      <td>Eu encontrei em cima do armário alguns potes c...</td>\n",
       "      <td>https://storage.googleapis.com/ciclos-10698-bu...</td>\n",
       "      <td>MEC/Rc7dMxTP7ZdLNEvmF0jo/r3DUtFJn9twSvqcNloNM/...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28/03/2022\\n[T] A cobra felena\\nem uma tarde m...</td>\n",
       "      <td>Eu encontrei em cima do armário alguns potes c...</td>\n",
       "      <td>https://storage.googleapis.com/ciclos-10698-bu...</td>\n",
       "      <td>MEC/Rc7dMxTP7ZdLNEvmF0jo/tKZIn59KtOcbe9z0n6oY/...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  ( O chorrinho nino                  )\\n-\\nEu e...   \n",
       "1  .As meninas do potes de Tintas [T]\\nUma vez eu...   \n",
       "2  1 uma menina encontrou varios potes de tinta  ...   \n",
       "3  2011 [T] A menina da (artes). \\n\\nDePois que e...   \n",
       "4  28/03/2022\\n[T] A cobra felena\\nem uma tarde m...   \n",
       "\n",
       "                                motivating_situation  \\\n",
       "0  Eu encontrei em cima do armário alguns potes c...   \n",
       "1  Eu encontrei em cima do armário alguns potes c...   \n",
       "2  Eu encontrei em cima do armário alguns potes c...   \n",
       "3  Eu encontrei em cima do armário alguns potes c...   \n",
       "4  Eu encontrei em cima do armário alguns potes c...   \n",
       "\n",
       "                                           image_url  \\\n",
       "0  https://storage.googleapis.com/ciclos-10698-bu...   \n",
       "1  https://storage.googleapis.com/ciclos-10698-bu...   \n",
       "2  https://storage.googleapis.com/ciclos-10698-bu...   \n",
       "3  https://storage.googleapis.com/ciclos-10698-bu...   \n",
       "4  https://storage.googleapis.com/ciclos-10698-bu...   \n",
       "\n",
       "                                          image_path  cohesion  \\\n",
       "0  MEC/Rc7dMxTP7ZdLNEvmF0jo/iet1QFw2ARNkv9Hx6KGe/...         3   \n",
       "1  MEC/Rc7dMxTP7ZdLNEvmF0jo/F80gTOBoh2Lk5JtrLWUO/...         3   \n",
       "2  MEC/Rc7dMxTP7ZdLNEvmF0jo/HjnamZPzaZcDeEANF2IC/...         3   \n",
       "3  MEC/Rc7dMxTP7ZdLNEvmF0jo/r3DUtFJn9twSvqcNloNM/...         3   \n",
       "4  MEC/Rc7dMxTP7ZdLNEvmF0jo/tKZIn59KtOcbe9z0n6oY/...         4   \n",
       "\n",
       "   thematic_coherence  formal_register  text_typology  \n",
       "0                   3                3              4  \n",
       "1                   3                3              4  \n",
       "2                   3                3              4  \n",
       "3                   3                3              4  \n",
       "4                   1                4              4  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATASET, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>motivating_situation</th>\n",
       "      <th>image_url</th>\n",
       "      <th>image_path</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>thematic_coherence</th>\n",
       "      <th>formal_register</th>\n",
       "      <th>text_typology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>( O chorrinho nino                  )\\n-\\nEu e...</td>\n",
       "      <td>Eu encontrei em cima do armário alguns potes c...</td>\n",
       "      <td>https://storage.googleapis.com/ciclos-10698-bu...</td>\n",
       "      <td>../data/MEC/Rc7dMxTP7ZdLNEvmF0jo/iet1QFw2ARNkv...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.As meninas do potes de Tintas [T]\\nUma vez eu...</td>\n",
       "      <td>Eu encontrei em cima do armário alguns potes c...</td>\n",
       "      <td>https://storage.googleapis.com/ciclos-10698-bu...</td>\n",
       "      <td>../data/MEC/Rc7dMxTP7ZdLNEvmF0jo/F80gTOBoh2Lk5...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 uma menina encontrou varios potes de tinta  ...</td>\n",
       "      <td>Eu encontrei em cima do armário alguns potes c...</td>\n",
       "      <td>https://storage.googleapis.com/ciclos-10698-bu...</td>\n",
       "      <td>../data/MEC/Rc7dMxTP7ZdLNEvmF0jo/HjnamZPzaZcDe...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011 [T] A menina da (artes). \\n\\nDePois que e...</td>\n",
       "      <td>Eu encontrei em cima do armário alguns potes c...</td>\n",
       "      <td>https://storage.googleapis.com/ciclos-10698-bu...</td>\n",
       "      <td>../data/MEC/Rc7dMxTP7ZdLNEvmF0jo/r3DUtFJn9twSv...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28/03/2022\\n[T] A cobra felena\\nem uma tarde m...</td>\n",
       "      <td>Eu encontrei em cima do armário alguns potes c...</td>\n",
       "      <td>https://storage.googleapis.com/ciclos-10698-bu...</td>\n",
       "      <td>../data/MEC/Rc7dMxTP7ZdLNEvmF0jo/tKZIn59KtOcbe...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  ( O chorrinho nino                  )\\n-\\nEu e...   \n",
       "1  .As meninas do potes de Tintas [T]\\nUma vez eu...   \n",
       "2  1 uma menina encontrou varios potes de tinta  ...   \n",
       "3  2011 [T] A menina da (artes). \\n\\nDePois que e...   \n",
       "4  28/03/2022\\n[T] A cobra felena\\nem uma tarde m...   \n",
       "\n",
       "                                motivating_situation  \\\n",
       "0  Eu encontrei em cima do armário alguns potes c...   \n",
       "1  Eu encontrei em cima do armário alguns potes c...   \n",
       "2  Eu encontrei em cima do armário alguns potes c...   \n",
       "3  Eu encontrei em cima do armário alguns potes c...   \n",
       "4  Eu encontrei em cima do armário alguns potes c...   \n",
       "\n",
       "                                           image_url  \\\n",
       "0  https://storage.googleapis.com/ciclos-10698-bu...   \n",
       "1  https://storage.googleapis.com/ciclos-10698-bu...   \n",
       "2  https://storage.googleapis.com/ciclos-10698-bu...   \n",
       "3  https://storage.googleapis.com/ciclos-10698-bu...   \n",
       "4  https://storage.googleapis.com/ciclos-10698-bu...   \n",
       "\n",
       "                                          image_path  cohesion  \\\n",
       "0  ../data/MEC/Rc7dMxTP7ZdLNEvmF0jo/iet1QFw2ARNkv...         3   \n",
       "1  ../data/MEC/Rc7dMxTP7ZdLNEvmF0jo/F80gTOBoh2Lk5...         3   \n",
       "2  ../data/MEC/Rc7dMxTP7ZdLNEvmF0jo/HjnamZPzaZcDe...         3   \n",
       "3  ../data/MEC/Rc7dMxTP7ZdLNEvmF0jo/r3DUtFJn9twSv...         3   \n",
       "4  ../data/MEC/Rc7dMxTP7ZdLNEvmF0jo/tKZIn59KtOcbe...         4   \n",
       "\n",
       "   thematic_coherence  formal_register  text_typology  \n",
       "0                   3                3              4  \n",
       "1                   3                3              4  \n",
       "2                   3                3              4  \n",
       "3                   3                3              4  \n",
       "4                   1                4              4  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"image_path\"] = df[\"image_path\"].apply(lambda image_path: os.path.join(ROOT, image_path))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1188 entries, 0 to 1187\n",
      "Data columns (total 8 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   text                  1188 non-null   object\n",
      " 1   motivating_situation  1188 non-null   object\n",
      " 2   image_url             1188 non-null   object\n",
      " 3   image_path            1188 non-null   object\n",
      " 4   cohesion              1188 non-null   int64 \n",
      " 5   thematic_coherence    1188 non-null   int64 \n",
      " 6   formal_register       1188 non-null   int64 \n",
      " 7   text_typology         1188 non-null   int64 \n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 83.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_truncated_images(df, column_name):\n",
    "    \"\"\"\n",
    "    Remove rows with truncated or corrupted images from the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing image file paths.\n",
    "    column_name (str): Column name containing image file paths.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with rows containing truncated or corrupted images removed.\n",
    "    \"\"\"\n",
    "    # Function to check if an image is valid\n",
    "    def is_valid_image(image_path):\n",
    "        with open(image_path, 'rb') as image_file:\n",
    "            check_chars = image_file.read()[-2:]\n",
    "        if check_chars != b'\\xff\\xd9':\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    # Apply the function to the DataFrame and filter out invalid images\n",
    "    valid_image_paths = df[column_name].apply(is_valid_image)\n",
    "    filtered_df = df[valid_image_paths].reset_index(drop=True)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "df = remove_truncated_images(df, \"image_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1163 entries, 0 to 1162\n",
      "Data columns (total 8 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   text                  1163 non-null   object\n",
      " 1   motivating_situation  1163 non-null   object\n",
      " 2   image_url             1163 non-null   object\n",
      " 3   image_path            1163 non-null   object\n",
      " 4   cohesion              1163 non-null   int64 \n",
      " 5   thematic_coherence    1163 non-null   int64 \n",
      " 6   formal_register       1163 non-null   int64 \n",
      " 7   text_typology         1163 non-null   int64 \n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 72.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"image\"] = df[\"image_path\"].progress_apply(lambda image_path: transform_image(cv2.imread(image_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbp_encoder = LBPEncoder()\n",
    "# features = lbp_encoder.fit_transform(df[\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf03af1aade4f36949b80872b73980c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clfs = [\n",
    "    SVC(random_state=SEED),\n",
    "    RandomForestClassifier(random_state=SEED),\n",
    "    DecisionTreeClassifier(random_state=SEED),\n",
    "    ExtraTreesClassifier(random_state=SEED),\n",
    "    XGBClassifier(\n",
    "        objective=\"multi:softprob\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "    ),\n",
    "    MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=SEED),\n",
    "]\n",
    "\n",
    "test_df = df.sample(n=100)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(action=\"ignore\", category=UndefinedMetricWarning)\n",
    "    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "    warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "    preprocessors = {\"text\": [clean_text], \"image\": [transform_image]}\n",
    "    cv_results = run_experiments(\n",
    "        clfs,\n",
    "        test_df,\n",
    "        cv=CV,\n",
    "        preprocessors=preprocessors,\n",
    "        text_feature_column=\"text\",\n",
    "        image_feature_column=\"image_path\",\n",
    "        target_column=COMPETENCE,\n",
    "        random_state=SEED\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algo</th>\n",
       "      <th>encoder</th>\n",
       "      <th>acc</th>\n",
       "      <th>macro_prec</th>\n",
       "      <th>weighted_prec</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>weighted_recall</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Support Vector</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.314167</td>\n",
       "      <td>0.532605</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.587425</td>\n",
       "      <td>0.087328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.287309</td>\n",
       "      <td>0.512776</td>\n",
       "      <td>0.314530</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.314530</td>\n",
       "      <td>0.574134</td>\n",
       "      <td>0.059093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.213842</td>\n",
       "      <td>0.468781</td>\n",
       "      <td>0.219322</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.219322</td>\n",
       "      <td>0.436725</td>\n",
       "      <td>-0.058877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.314167</td>\n",
       "      <td>0.532605</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.587425</td>\n",
       "      <td>0.087328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.331586</td>\n",
       "      <td>0.569246</td>\n",
       "      <td>0.331093</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.331093</td>\n",
       "      <td>0.580511</td>\n",
       "      <td>0.162572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.290015</td>\n",
       "      <td>0.520336</td>\n",
       "      <td>0.317460</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.317460</td>\n",
       "      <td>0.582842</td>\n",
       "      <td>0.085207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             algo encoder   acc  macro_prec  weighted_prec  macro_recall  \\\n",
       "0  Support Vector  TF-IDF  0.70    0.314167       0.532605      0.322222   \n",
       "1   Random Forest  TF-IDF  0.68    0.287309       0.512776      0.314530   \n",
       "2   Decision Tree  TF-IDF  0.42    0.213842       0.468781      0.219322   \n",
       "3     Extra Trees  TF-IDF  0.70    0.314167       0.532605      0.322222   \n",
       "4         XGBoost  TF-IDF  0.60    0.331586       0.569246      0.331093   \n",
       "5             MLP  TF-IDF  0.69    0.290015       0.520336      0.317460   \n",
       "\n",
       "   weighted_recall  macro_f1  weighted_f1     kappa  \n",
       "0             0.70  0.322222     0.587425  0.087328  \n",
       "1             0.68  0.314530     0.574134  0.059093  \n",
       "2             0.42  0.219322     0.436725 -0.058877  \n",
       "3             0.70  0.322222     0.587425  0.087328  \n",
       "4             0.60  0.331093     0.580511  0.162572  \n",
       "5             0.69  0.317460     0.582842  0.085207  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results_df = scores_to_df(cv_results)\n",
    "cv_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllrrrrrrrr}\n",
      "\\toprule\n",
      " & algo & encoder & acc & macro_prec & weighted_prec & macro_recall & weighted_recall & macro_f1 & weighted_f1 & kappa \\\\\n",
      "\\midrule\n",
      "0 & Support Vector & TF-IDF & 0.700000 & 0.314167 & 0.532605 & 0.322222 & 0.700000 & 0.322222 & 0.587425 & 0.087328 \\\\\n",
      "1 & Random Forest & TF-IDF & 0.680000 & 0.287309 & 0.512776 & 0.314530 & 0.680000 & 0.314530 & 0.574134 & 0.059093 \\\\\n",
      "2 & Decision Tree & TF-IDF & 0.420000 & 0.213842 & 0.468781 & 0.219322 & 0.420000 & 0.219322 & 0.436725 & -0.058877 \\\\\n",
      "3 & Extra Trees & TF-IDF & 0.700000 & 0.314167 & 0.532605 & 0.322222 & 0.700000 & 0.322222 & 0.587425 & 0.087328 \\\\\n",
      "4 & XGBoost & TF-IDF & 0.600000 & 0.331586 & 0.569246 & 0.331093 & 0.600000 & 0.331093 & 0.580511 & 0.162572 \\\\\n",
      "5 & MLP & TF-IDF & 0.690000 & 0.290015 & 0.520336 & 0.317460 & 0.690000 & 0.317460 & 0.582842 & 0.085207 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cv_results_df.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(ROOT, f\"{COMPETENCE}_baseline_scores.csv\")\n",
    "cv_results_df.to_csv(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal-aes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
